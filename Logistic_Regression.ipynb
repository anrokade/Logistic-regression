{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "tTC_UW6ghG0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "sKP50UnRhHGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ What is Logistic Regression?\n",
        "\n",
        "**Logistic Regression** is a supervised learning algorithm used for **classification tasks** üéØ ‚Äî most commonly binary classification (i.e., classifying data into two categories: yes/no, 0/1, spam/not spam, etc.).\n",
        "\n",
        "* It models the **probability** that a given input belongs to a particular category.\n",
        "* It uses the **logistic (sigmoid) function** to map predicted values to probabilities between 0 and 1:\n",
        "\n",
        "$$P(y = 1 | x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}$$\n",
        "\n",
        "* Based on a **threshold** (typically 0.5), it classifies the input.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What is Linear Regression?\n",
        "\n",
        "**Linear Regression** is a supervised learning algorithm used for **regression tasks**, where the output is a **continuous numeric value** üìà.\n",
        "\n",
        "* It tries to model the **linear relationship** between the independent variables (features) and the dependent variable (target):\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon$$\n",
        "\n",
        "* The goal is to minimize the error between the predicted values and the actual values, usually using **least squares**.\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ Key Differences\n",
        "\n",
        "| Feature | Linear Regression | Logistic Regression |\n",
        "| :--- | :--- | :--- |\n",
        "| **Type of Problem** | Regression (predicting continuous values) | Classification (predicting categories) |\n",
        "| **Output** | Real number | Probability (0 to 1) |\n",
        "| **Function Used** | Linear function | Sigmoid/logistic function |\n",
        "| **Prediction** | Direct numerical value | Probability ‚Üí classified into 0 or 1 |\n",
        "| **Loss Function** | Mean Squared Error (MSE) | Log Loss / Binary Cross-Entropy |\n",
        "\n"
      ],
      "metadata": {
        "id": "W0g7GmEkhHM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.  Explain the role of the Sigmoid function in Logistic Regression"
      ],
      "metadata": {
        "id": "iAK1bem5hHP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Role of the Sigmoid Function in Logistic Regression\n",
        "\n",
        "The **sigmoid function** plays a central role in Logistic Regression by converting the output of a linear model into a **probability score**, which is crucial for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è What is the Sigmoid Function?\n",
        "\n",
        "The sigmoid (or logistic) function is a mathematical function that maps any real-valued number to a value between 0 and 1.\n",
        "\n",
        "[Image of Sigmoid function]\n",
        "\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where $z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n$ is the linear combination of inputs.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Why is it Used in Logistic Regression?\n",
        "\n",
        "Logistic Regression first computes a linear combination of the input features, like in Linear Regression, but instead of outputting that value directly, it passes it through the sigmoid function to get a probability:\n",
        "\n",
        "$$\n",
        "P(y = 1 | x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "This ensures the output is always between 0 and 1 and is interpretable as a probability.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Classification Using the Sigmoid Output\n",
        "\n",
        "After calculating the probability $P(y=1|x)$, we can make predictions based on a threshold (typically 0.5):\n",
        "\n",
        "* If $P \\ge 0.5$, predict class 1.\n",
        "* If $P < 0.5$, predict class 0.\n",
        "\n",
        "You can change this threshold depending on the specific application's needs.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Visual Intuition\n",
        "\n",
        "The sigmoid curve has an \"S\" shape. As the linear output $z$ gets larger, the function's value approaches 1. As $z$ gets smaller, the value approaches 0. At $z=0$, the value is exactly 0.5. This smooth transition makes it ideal for modeling probabilities in binary classification.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "| Feature | Description |\n",
        "| :--- | :--- |\n",
        "| **Function** | $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ |\n",
        "| **Purpose** | Converts linear output to a probability |\n",
        "| **Output Range** | Between 0 and 1 |\n",
        "| **Used For** | Binary classification decisions |\n",
        "| **Why Not Linear Output?**| Linear output can exceed [0,1], which is not valid as a probability. |"
      ],
      "metadata": {
        "id": "8z7BKQ14hHTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is Regularization in Logistic Regression and why is it needed?"
      ],
      "metadata": {
        "id": "ac7IKsWthHW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîê What is Regularization in Logistic Regression?\n",
        "\n",
        "**Regularization** is a technique used in logistic regression to **prevent overfitting** by penalizing large coefficients (weights) in the model. This encourages simpler models that generalize better to new, unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### üö® Why is Regularization Needed?\n",
        "\n",
        "In real-world datasets, models can become too complex and might fit the training data too well, even capturing noise as if it were a meaningful pattern. This is known as **overfitting**, and it leads to poor performance on new data. Regularization combats this by controlling the model's complexity, discouraging it from assigning overly large weights to specific features.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß How Regularization Works\n",
        "\n",
        "Regularization adds a penalty term to the model's loss function. The model then tries to minimize this new, combined loss function.\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\text{Log Loss} + \\lambda \\cdot \\text{Regularization Term}\n",
        "$$\n",
        "\n",
        "* **Log Loss:** The original loss function of logistic regression, which measures the model's prediction error.\n",
        "* **Regularization Term:** The penalty for large coefficients.\n",
        "* **$\\lambda$ (Lambda):** A hyperparameter that controls the strength of the regularization. A larger $\\lambda$ means a stronger penalty, leading to smaller coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Types of Regularization\n",
        "\n",
        "| Type | Formula | Effect on Weights | Common Use |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **L1 (Lasso)** | $\\sum |\\beta_j|$ | Can shrink some coefficients to exactly zero, performing **feature selection**. | When you want a simpler, more interpretable model with fewer features. |\n",
        "| **L2 (Ridge)** | $\\sum \\beta_j^2$ | Shrinks all coefficients toward zero, but doesn't make them exactly zero. | When all features are relevant and you want a more stable model. |\n",
        "| **Elastic Net** | Combination of L1 and L2 | Provides a balance between L1's sparsity and L2's stability. | In cases where you have many features and some may be correlated. |"
      ],
      "metadata": {
        "id": "DB3rkn6ihHZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.What are some common evaluation metrics for classification models, and why are they important?"
      ],
      "metadata": {
        "id": "btxrMXsmhHcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Common Evaluation Metrics for Classification Models ‚Äî and Why They Matter\n",
        "\n",
        "Evaluation metrics are essential for understanding how well a classification model performs, especially when dealing with imbalanced datasets or different costs for different types of errors.\n",
        "\n",
        "***\n",
        "\n",
        "### ‚úÖ 1. Accuracy\n",
        "\n",
        "* **Definition:** The ratio of correctly predicted observations to the total observations.\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "* **Use When:** Classes are balanced (an equal number of 0s and 1s).\n",
        "* **Limitation:** Can be misleading if the data is imbalanced (e.g., 95% of one class).\n",
        "\n",
        "***\n",
        "\n",
        "### üîÅ 2. Precision\n",
        "\n",
        "* **Definition:** Out of all predicted positives, how many were actually positive?\n",
        "    $$\n",
        "    \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "    $$\n",
        "* **Use When:** False positives are costly (e.g., spam detection, cancer diagnosis).\n",
        "\n",
        "***\n",
        "\n",
        "### üîé 3. Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "* **Definition:** Out of all actual positives, how many did the model correctly identify?\n",
        "    $$\n",
        "    \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "    $$\n",
        "* **Use When:** False negatives are costly (e.g., fraud detection, medical tests).\n",
        "\n",
        "***\n",
        "\n",
        "### ‚öñÔ∏è 4. F1 Score\n",
        "\n",
        "* **Definition:** The harmonic mean of precision and recall. It balances both.\n",
        "    $$\n",
        "    \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "    $$\n",
        "* **Use When:** You need a balance between precision and recall, especially when the dataset is imbalanced.\n",
        "\n",
        "***\n",
        "\n",
        "### üìà 5. ROC Curve and AUC (Area Under the Curve)\n",
        "\n",
        "* **ROC Curve:** Plots the True Positive Rate vs. the False Positive Rate at various classification thresholds.\n",
        "* **AUC:** Measures the entire two-dimensional area under the ROC curve.\n",
        "    $$\n",
        "    \\text{AUC} = 1.0 \\text{ (Perfect)} \\quad \\text{AUC} = 0.5 \\text{ (Random guessing)}\n",
        "    $$\n",
        "* **Use When:** Comparing different models or evaluating performance across various thresholds.\n",
        "\n",
        "***\n",
        "\n",
        "### üî¢ 6. Confusion Matrix\n",
        "\n",
        "A 2x2 matrix that shows:\n",
        "* True Positives (TP)\n",
        "* True Negatives (TN)\n",
        "* False Positives (FP)\n",
        "* False Negatives (FN)\n",
        "* **Purpose:** Helps visualize and derive all other metrics.\n",
        "\n",
        "***\n",
        "\n",
        "### üéØ Why Are These Metrics Important?\n",
        "\n",
        "| Metric | Tells You About | Importance When... |\n",
        "| :--- | :--- | :--- |\n",
        "| **Accuracy** | Overall correctness | Classes are balanced. |\n",
        "| **Precision** | False positive control | The cost of a false alarm is high. |\n",
        "| **Recall** | False negative control | Missing a positive case is dangerous. |\n",
        "| **F1 Score** | Precision/Recall balance | You need a trade-off or have class imbalance. |\n",
        "| **AUC-ROC** | Model discrimination | Comparing models and evaluating threshold behavior. |\n",
        "\n",
        "I'm unable to directly convert the provided text into a Google Colab Markdown format. However, I can provide the information in a clean, Markdown-formatted text that would be suitable for copying and pasting into a Colab notebook's text cells.\n",
        "\n",
        "### ‚úÖ Example Use Cases:\n",
        "\n",
        "* **Medical Diagnosis:** Prioritize **Recall** ‚öïÔ∏è. This is because a false negative (failing to detect a disease) can have severe consequences. It is better to have a few false positives (flagging a healthy person as sick) and run further tests than to miss a true case.\n",
        "* **Email Spam Filter:** Prioritize **Precision** üìß. A false positive in this case means a legitimate email is marked as spam. Users find it more frustrating to have important emails sent to the spam folder than to have a few spam emails get into their inbox.\n",
        "* **Search Ranking:** Use **AUC** or **F1** üîé. In this scenario, you need a balanced metric that considers both relevant and irrelevant results. The AUC provides an overall measure of a model's ability to discriminate between classes, while the F1 score offers a balance between precision and recall, which is often crucial for search relevance."
      ],
      "metadata": {
        "id": "BJ9JaBvjhHfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Write a Python program that loads a CSV file into a Pandas DataFrame,splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "q3Ze9G90hHkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Logistic Regression model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvdlQa4Zp2yB",
        "outputId": "738248f7-4bd2-48ce-ebcf-9596e77a134f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy. (Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "pJJa0lyuhHnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Convert to a binary classification problem (e.g., classifying if species is 'setosa')\n",
        "# 'setosa' = 0, others = 1\n",
        "y_binary = (y != 0).astype(int)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression model with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy on test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r654h4fKwBXm",
        "outputId": "46797377-208c-48ae-c033-97cdef10c258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "sepal length (cm): 0.4276\n",
            "sepal width (cm): -0.8877\n",
            "petal length (cm): 2.2147\n",
            "petal width (cm): 0.9161\n",
            "\n",
            "Accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report. (Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "k2wvdqdXwNGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Wrap Logistic Regression with OneVsRestClassifier\n",
        "model = OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=1000))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vssyijDAwCac",
        "outputId": "484f2a77-5f70-4a31-de2c-c42ff32de582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "K9bnlF3M0Yr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (optional if only tuning)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'estimator__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'estimator__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Base model with solver that supports both L1 and L2\n",
        "base_model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Wrap with OneVsRestClassifier for multi-class\n",
        "model = OneVsRestClassifier(base_model)\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=model,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and accuracy\n",
        "print(\"‚úÖ Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"‚úÖ Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0bH_llIw6L3",
        "outputId": "4e3adc15-cb34-46e1-e37c-0a5763ce8d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Best Parameters: {'estimator__C': 10, 'estimator__penalty': 'l1'}\n",
            "‚úÖ Best Cross-Validation Accuracy: 0.9583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.(Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "44c6VPJL0lWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Ignore convergence and deprecation warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------\n",
        "# Model WITHOUT Scaling\n",
        "# ------------------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ------------------------------\n",
        "# Model WITH Scaling\n",
        "# ------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ------------------------------\n",
        "# Results\n",
        "# ------------------------------\n",
        "print(f\"üî∏ Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"‚úÖ Accuracy with scaling   : {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm6ONM4Yx1io",
        "outputId": "12700300-6c81-40d3-8cc6-4798d17960ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî∏ Accuracy without scaling: 1.0000\n",
            "‚úÖ Accuracy with scaling   : 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.  Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you‚Äôd take to build a Logistic Regression model ‚Äî including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case."
      ],
      "metadata": {
        "id": "wgRV1qv-2L4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression for Imbalanced Marketing Response Prediction\n",
        "\n",
        "Imagine you work at an e-commerce company predicting which customers will respond to a marketing campaign. The dataset is highly imbalanced: only 5% respond. Here's a recommended approach:\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Data Understanding & Preprocessing\n",
        "\n",
        "- Explore the dataset and target distribution (5% responders = heavy imbalance).  \n",
        "- Clean missing values, handle outliers, and inconsistencies.  \n",
        "- Feature engineering: e.g., customer recency, frequency, monetary value, demographics, past campaign interactions.\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Feature Scaling\n",
        "\n",
        "- Use `StandardScaler` or `MinMaxScaler` on numeric features.  \n",
        "- Helps Logistic Regression converge faster and improves regularization.\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Handling Class Imbalance\n",
        "\n",
        "- **Resampling Techniques:**  \n",
        "  - Oversample minority class with SMOTE or RandomOverSampler.  \n",
        "  - Undersample majority class (caution: may lose info).  \n",
        "  - Combine oversampling and undersampling if needed.\n",
        "\n",
        "- **Class Weights:**  \n",
        "  - Use `class_weight='balanced'` in Logistic Regression to penalize errors on minority class more.  \n",
        "  - Helps without altering data distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Train/Test Split\n",
        "\n",
        "- Use **stratified splitting** to keep imbalance ratio consistent in train and test sets.\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Model Building & Hyperparameter Tuning\n",
        "\n",
        "- Train Logistic Regression with regularization (L1 or L2).  \n",
        "- Tune hyperparameters via `GridSearchCV` or `RandomizedSearchCV`:  \n",
        "  - `C` (inverse regularization strength)  \n",
        "  - `penalty` (`l1` or `l2`)  \n",
        "  - Solver compatible with chosen penalty  \n",
        "  - Optionally, `class_weight` if not using resampling.\n",
        "\n",
        "- Tune classification **thresholds** since default 0.5 may not be optimal.\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Evaluation Metrics\n",
        "\n",
        "> Accuracy is misleading with imbalanced data. Use:  \n",
        "\n",
        "- **Precision, Recall, F1-Score** (balance false positives vs false negatives).  \n",
        "- **ROC-AUC** (overall separability).  \n",
        "- **Precision-Recall Curve / AUPRC** (focus on minority class performance).  \n",
        "- **Confusion Matrix** (detailed TP, FP, TN, FN).  \n",
        "- **Business metrics** like expected profit or lift from targeting predicted responders.\n",
        "\n",
        "---\n",
        "\n",
        "## 7Ô∏è‚É£ Model Interpretation\n",
        "\n",
        "- Analyze coefficients to understand feature impact.  \n",
        "- Use SHAP or LIME for explainability if needed.\n",
        "\n",
        "---\n",
        "\n",
        "## 8Ô∏è‚É£ Deployment & Monitoring\n",
        "\n",
        "- Deploy model in production pipeline.  \n",
        "- Monitor real-time performance and retrain as customer behavior changes.  \n",
        "- Track campaign ROI to ensure business value.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Step                  | Description                                           |\n",
        "|-----------------------|-----------------------------------------------------|\n",
        "| Data preprocessing    | Clean, feature engineer, scale                       |\n",
        "| Handling imbalance    | Class weighting and/or resampling                    |\n",
        "| Model tuning          | Grid search `C`, `penalty`, `class_weight`          |\n",
        "| Metrics & evaluation  | Precision, recall, F1, AUC, PR curves                |\n",
        "| Business alignment    | Connect predictions to marketing KPIs and profit    |\n",
        "\n"
      ],
      "metadata": {
        "id": "1c4FB44-4XNy"
      }
    }
  ]
}